<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Less is More: Adapting Text Embeddings for LRLs</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/water.css@2/out/water.css">
    <style>
        body { max-width: 900px; margin: 0 auto; padding: 20px; line-height: 1.6; }
        header { text-align: center; border-bottom: 2px solid #efefef; padding-bottom: 20px; margin-bottom: 40px; }
        .authors { font-size: 1.1em; color: #555; }
        .affiliations { font-style: italic; color: #777; }
        .abstract { background: #f9f9f9; padding: 20px; border-radius: 8px; border-left: 5px solid #0076ff; }
        table { width: 100%; border-collapse: collapse; margin: 20px 0; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2 f2 f2; }
        .nav-links { display: flex; justify-content: center; gap: 20px; margin-bottom: 30px; font-weight: bold; }
    </style>
</head>
<body>

<header>
    <h1>Less is More: Adapting Text Embeddings for Low-Resource Languages with Small Scale Noisy Synthetic Data</h1>
    [cite_start]<div class="authors">Zaruhi Navasardyan, Spartak Bughdaryan, Bagrat Minasyan, Hrant Davtyan [cite: 2]</div>
    [cite_start]<div class="affiliations">Metric AI Lab [cite: 3]</div>
</header>

<nav class="nav-links">
    <a href="#abstract">Abstract</a>
    <a href="#introduction">Introduction</a>
    <a href="#results">Results</a>
    <a href="#conclusion">Conclusion</a>
</nav>

<section id="abstract" class="abstract">
    <h2>Abstract</h2>
    [cite_start]<p>This work challenges the assumption that effective semantic alignment for low-resource languages (LRLs) requires massive datasets[cite: 7]. [cite_start]Focusing on Armenian, we show that fine-tuning a multilingual encoder on just 10,000 noisy synthetic pairs yields 11-12% average improvements, matching the performance of models trained on 1 million examples[cite: 8, 10].</p>
</section>

<section id="introduction">
    <h2>1. Introduction</h2>
    [cite_start]<p>Text embeddings are the backbone of modern RAG systems and semantic search[cite: 16]. [cite_start]While high-resource languages enjoy an abundance of benchmarks, LRLs like Armenian remain underserved[cite: 17]. [cite_start]We explore a minimalist approach: leveraging noisy synthetic data generated via LLM translation to fine-tune strong multilingual base models[cite: 20].</p>
</section>

<section id="method">
    <h2>2. Method</h2>
    [cite_start]<p>We generated training data by translating English Reddit title-body pairs into Armenian using Gemma-2-27B-it[cite: 51, 53]. [cite_start]Although the translations were often grammatically incorrect, they preserved the contextual meaning necessary for semantic alignment[cite: 54, 55].</p>
</section>

<section id="results">
    <h2>3. Main Results</h2>
    [cite_start]<p>Fine-tuning the mE5 model on just 10k examples led to a 35% relative improvement in retrieval tasks[cite: 120]. [cite_start]The performance remained nearly identical even when scaling the data to 1 million pairs[cite: 136, 180].</p>
    
    <h3>Baseline Performance (Table 1)</h3>
    <table>
        <tr>
            <th>Model</th>
            <th>Retrieval</th>
            <th>STS [hye]</th>
            <th>Average</th>
        </tr>
        <tr>
            <td>multilingual-e5-base</td>
            <td>58.15</td>
            <td>66.19</td>
            <td>64.30</td>
        </tr>
        <tr>
            <td>multilingual-e5-large-it</td>
            <td>73.37</td>
            <td>69.94</td>
            <td>72.99</td>
        </tr>
    </table>
    [cite_start]<p><i>[cite: 95]</i></p>
</section>

<section id="conclusion">
    <h2>4. Conclusion</h2>
    [cite_start]<p>Our findings suggest that for strong multilingual baselines, semantic structure is already latent; the primary challenge is merely surface-level alignment[cite: 173, 174]. [cite_start]This democratizes high-performance embedding creation for resource-constrained communities[cite: 175].</p>
</section>

<footer>
    <hr>
    <p style="text-align: center;">Â© 2026 Metric AI Lab. View the full paper <a href="ATE_2_for_LoResLm__EACL_2026.pdf">here</a>.</p>
</footer>

</body>
</html>
