<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Less is More: Adapting Text Embeddings for LRLs</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/water.css@2/out/water.css">
    <style>
        body { max-width: 900px; margin: 0 auto; padding: 20px; line-height: 1.6; }
        header { text-align: center; margin-bottom: 40px; }
        .authors { font-size: 1.1em; opacity: 0.9; }
        .affiliations { font-style: italic; opacity: 0.7; margin-bottom: 10px; }
        .abstract-box { border-left: 5px solid #0076ff; padding: 10px 20px; margin: 20px 0; background: rgba(128, 128, 128, 0.05); }
        table { width: 100%; margin: 20px 0; border-collapse: collapse; }
        nav { display: flex; justify-content: center; gap: 15px; flex-wrap: wrap; margin-bottom: 40px; border-bottom: 1px solid rgba(128, 128, 128, 0.2); padding-bottom: 20px; }
        footer { margin-top: 50px; text-align: center; font-size: 0.9em; opacity: 0.6; }
        .stat-highlight { font-weight: bold; color: #0076ff; }
    </style>
</head>
<body>

<header>
    <h1>Less is More: Adapting Text Embeddings for Low-Resource Languages with Small Scale Noisy Synthetic Data</h1>
    [cite_start]<div class="authors">Zaruhi Navasardyan, Spartak Bughdaryan, Bagrat Minasyan, Hrant Davtyan [cite: 2]</div>
    [cite_start]<div class="affiliations">Metric AI Lab [cite: 3]</div>
    [cite_start]<p>Presented at EACL 2026 [cite: 1]</p>
</header>

<nav>
    <a href="#abstract">Abstract</a>
    <a href="#introduction">Introduction</a>
    <a href="#method">Method</a>
    <a href="#results">Key Results</a>
    <a href="#conclusion">Conclusion</a>
</nav>

<section id="abstract">
    <h2>Abstract</h2>
    <div class="abstract-box">
        [cite_start]<p>Low-resource languages (LRLs) often lack high-quality, large-scale datasets for training effective text embedding models[cite: 6]. [cite_start]We challenge the assumption that effective semantic alignment requires massive datasets or pristine translations[cite: 7]. [cite_start]By fine-tuning a multilingual encoder (mE5) on just <span class="stat-highlight">10,000 noisy synthetic pairs</span>, we achieved an 11-12% average improvement across benchmarks, matching the performance of models trained on 1 million examples[cite: 10].</p>
    </div>
</section>

<section id="introduction">
    <h2>1. Introduction</h2>
    [cite_start]<p>Text embeddings are critical for Retrieval-Augmented Generation (RAG) and semantic search[cite: 16]. [cite_start]While high-resource languages like English have massive corpora, languages like Armenian (hye) remain underserved[cite: 17]. [cite_start]We demonstrate that semantic alignment for LRLs saturates early and is highly robust to noise[cite: 13].</p>
</section>

<section id="method">
    <h2>2. Methodology</h2>
    <ul>
        [cite_start]<li><strong>Data Source:</strong> We sampled 2 million Reddit (title, body) pairs[cite: 51].</li>
        [cite_start]<li><strong>Translation:</strong> Pairs were translated to Armenian using <em>Gemma-2-27B-it</em>[cite: 53].</li>
        [cite_start]<li><strong>Noise Profile:</strong> Translations were often imperfect or grammatically incorrect, yet they preserved contextual meaning[cite: 54].</li>
        [cite_start]<li><strong>Filtering:</strong> We used semantic consistency metrics (Semantic Drift and Translation Drift) to prune the corpus to 1 million high-fidelity samples[cite: 73, 81].</li>
    </ul>
</section>

<section id="results">
    <h2>3. Key Results</h2>
    [cite_start]<p>Scaling data from 10k to 1M examples yielded only marginal gains, suggesting that alignment happens rapidly with minimal data[cite: 136, 138].</p>
    
    <h3>Model Performance Comparison</h3>
    <table>
        <thead>
            <tr>
                <th>Model</th>
                <th>Retrieval</th>
                <th>STS [hye]</th>
                <th>Average Score</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>multilingual-e5-base (Baseline)</td>
                <td>58.15</td>
                <td>66.19</td>
                [cite_start]<td>64.30 [cite: 95]</td>
            </tr>
            <tr>
                <td><strong>Our Model (10k samples)</strong></td>
                <td><strong>79.35</strong></td>
                <td><strong>70.84</strong></td>
                [cite_start]<td><strong>76.42 [cite: 130]</strong></td>
            </tr>
            <tr>
                <td>Our Model (1M samples)</td>
                <td>83.70</td>
                <td>69.04</td>
                [cite_start]<td>73.92 [cite: 130]</td>
            </tr>
        </tbody>
    </table>
</section>

<section id="conclusion">
    <h2>4. Conclusion</h2>
    [cite_start]<p>Our "Less is More" finding suggests that for strong multilingual baselines, the semantic structure is already latent; the challenge is merely surface-level alignment[cite: 173, 174]. [cite_start]This approach democratizes SOTA embedding creation for communities with limited compute and data resources[cite: 175].</p>
</section>

<footer>
    [cite_start]<p>Full Model, Data, and Benchmark released to the public. [cite: 14]</p>
    <p>Â© 2026 Metric AI Lab</p>
</footer>

</body>
</html>
